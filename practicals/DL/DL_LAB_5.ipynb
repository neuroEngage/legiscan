{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCi_ymKLvtdm"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h37HOobCGYqL"
      },
      "outputs": [],
      "source": [
        "# Create a sample graph\n",
        "G = nx.Graph()\n",
        "G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)])  # Adding connections\n",
        "\n",
        "# Assign sample text data to nodes\n",
        "G.nodes[1]['text'] = \"This product is amazing!\"\n",
        "G.nodes[2]['text'] = \"this is bad.\"\n",
        "G.nodes[3]['text'] = \"It's not great.\"\n",
        "G.nodes[4]['text'] = \"I love it! Highly recommend.\"\n",
        "G.nodes[5]['text'] = \"i hate it.\"\n",
        "G.nodes[6]['text'] = \"everything is awesome.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEGQ2G2IPVBM"
      },
      "outputs": [],
      "source": [
        "#pre processing data\n",
        "\n",
        "# Extract text data from nodes\n",
        "texts = [G.nodes[n]['text'] for n in G.nodes]\n",
        "\n",
        "max_words = 5000\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words = max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXtW6cfWSqIO",
        "outputId": "fc635355-efb0-4491-8109-9ab19a5c9307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.6667 - loss: 0.6931\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 0.6926\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.6667 - loss: 0.6921\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.6916\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8333 - loss: 0.6911\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.6667 - loss: 0.6906\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6667 - loss: 0.6900\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6667 - loss: 0.6892\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8333 - loss: 0.6884\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8333 - loss: 0.6874\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.6862\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.6849\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.6834\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6816\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6797\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.6775\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.6750\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.6721\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.6688\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6650\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cb7c7a1c0d0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Build & Train an RNN (LSTM Model)python\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 16\n",
        "output_dim = 1  # Binary classification (positive/negative)\n",
        "\n",
        "# Create RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_length),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(output_dim, activation=\"sigmoid\")])  # Sigmoid for binary classification\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Dummy labels (1 for positive, 0 for negative)\n",
        "import numpy as np\n",
        "labels = [1, 0, 0, 1, 0, 1]\n",
        "padded_sequences = np.array(padded_sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Train Model\n",
        "model.fit(padded_sequences, labels, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZB63JInYDjC",
        "outputId": "e90151b8-7999-4167-9d10-f56ab9f85f27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cb7c78f3e20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-35-1165e659d541>:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  G.nodes[node]['sentiment'] = float(sentiment_scores[i])  # Assigning sentiment score\n"
          ]
        }
      ],
      "source": [
        "# Predict sentiment scores for each node\n",
        "sentiment_scores = model.predict(padded_sequences)\n",
        "for i, node in enumerate(G.nodes):\n",
        "    G.nodes[node]['sentiment'] = float(sentiment_scores[i])  # Assigning sentiment score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBHnmStAYEVy",
        "outputId": "bdc9f696-ef34-4a52-fc5e-526ad79152b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive Sentiment Nodes: [1, 4, 6]\n",
            "Negative Sentiment Nodes: [2, 3, 5]\n"
          ]
        }
      ],
      "source": [
        "# Analyze sentiment-based communities\n",
        "positive_nodes = [n for n in G.nodes if G.nodes[n]['sentiment'] > 0.5]\n",
        "negative_nodes = [n for n in G.nodes if G.nodes[n]['sentiment'] <= 0.5]\n",
        "\n",
        "print(\"Positive Sentiment Nodes:\", positive_nodes)\n",
        "print(\"Negative Sentiment Nodes:\", negative_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explaination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "1. Import Libraries:\n",
        "    NumPy: Used for handling arrays and numerical operations.\n",
        "\n",
        "    NetworkX: Used for creating and analyzing graph structures.\n",
        "\n",
        "    TensorFlow: A deep learning library to build and train the LSTM model.\n",
        "\n",
        "    Tokenizer & Pad Sequences: From Keras, used for text preprocessing.\n",
        "\n",
        "2. Create a Graph:\n",
        "    A graph G is created using networkx.Graph(). This graph represents a collection of nodes and edges.\n",
        "\n",
        "    Edges are added using G.add_edges_from(). In this case, nodes 1 through 6 are connected in a linear fashion.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    (1) -- (2) -- (3) -- (4) -- (5) -- (6)\n",
        "3. Assign Sample Text Data to Nodes:\n",
        "    Each node (from 1 to 6) is assigned a piece of text that represents some opinion or sentiment about a product.\n",
        "\n",
        "    The text data will be used for sentiment analysis to classify the opinions as positive or negative.\n",
        "\n",
        "4. Text Preprocessing:\n",
        "    Extracting Text Data: A list of text from all nodes is created.\n",
        "\n",
        "    Tokenization: Tokenizer from Keras is used to convert the text data into numerical sequences. \n",
        "    This step transforms each word into a unique integer based on its frequency in the dataset.\n",
        "\n",
        "    Padding Sequences: The sequences are padded to ensure that all inputs to the model have the same length. \n",
        "    This is done by pad_sequences(), ensuring that shorter sequences are padded with zeros at the end.\n",
        "\n",
        "5. Build & Train the RNN Model:\n",
        "    Model Architecture:\n",
        "\n",
        "    Embedding Layer: Converts input words into dense vectors of fixed size (16 here). The words are represented in a continuous vector space where similar words have similar vector representations.\n",
        "\n",
        "    LSTM Layers: The Long Short-Term Memory (LSTM) layers are used to capture dependencies in the sequence of words. There are two LSTM layers:\n",
        "\n",
        "    The first LSTM layer returns sequences of outputs (to feed into the next LSTM layer).\n",
        "\n",
        "    The second LSTM layer outputs a single vector (for further dense layers).\n",
        "\n",
        "    Dense Layers: The model has dense layers that perform computations after the LSTM layers, and finally, a sigmoid activation function is used to output a probability (0 to 1) for binary classification.\n",
        "\n",
        "    Compiling the Model:\n",
        "\n",
        "    Loss Function: binary_crossentropy is used because we are dealing with binary classification (positive/negative sentiment).\n",
        "\n",
        "    Optimizer: adam, a popular optimizer for training deep learning models.\n",
        "\n",
        "    Metrics: accuracy to measure the model's performance.\n",
        "\n",
        "    Training the Model: The model is trained using the text data (padded sequences) and labels (0 for negative, 1 for positive sentiment). It is trained for 20 epochs.\n",
        "\n",
        "6. Prediction and Sentiment Assignment:\n",
        "    Prediction: After training, the model predicts sentiment scores (between 0 and 1) for each node's text. This score represents the sentiment of the text assigned to the node.\n",
        "\n",
        "    Assign Sentiment to Nodes: The predicted sentiment score for each node is stored as an attribute in the graph. Nodes with sentiment scores greater than 0.5 are classified as \"positive\" sentiment, while nodes with scores 0.5 or lower are classified as \"negative\".\n",
        "\n",
        "7. Sentiment-Based Community Analysis:\n",
        "    Positive Nodes: All nodes with a sentiment score greater than 0.5 are classified as having a positive sentiment.\n",
        "\n",
        "    Negative Nodes: All nodes with a sentiment score less than or equal to 0.5 are classified as having a negative sentiment.\n",
        "\n",
        "    The positive and negative nodes are then printed out.\n",
        "\n",
        "    Output:\n",
        "    The code prints the nodes of the graph that have positive and negative sentiment based on their sentiment scores. For example:\n",
        "\n",
        "    Positive Sentiment Nodes: [1, 4, 6]\n",
        "    Negative Sentiment Nodes: [2, 3, 5]\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
